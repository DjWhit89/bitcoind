\chapter{libmultiprocess Design }
\label{md_src_2ipc_2libmultiprocess_2doc_2design}\index{libmultiprocess Design@{libmultiprocess Design}}
\label{md_src_2ipc_2libmultiprocess_2doc_2design_autotoc_md14}%


Given an interface description of an object with one or more methods, libmultiprocess generates\+:


\begin{DoxyItemize}
\item A C++ {\ttfamily Proxy\+Client} class template specialization with an implementation of each interface method that sends a request over a socket, waits for a response, and returns the result.
\item A C++ {\ttfamily Proxy\+Server} class template specialization that listens for requests over a socket and calls a wrapped C++ object implementing the same interface to actually execute the requests.
\end{DoxyItemize}

The function call â‡† request translation supports input and output arguments, standard types like {\ttfamily unique\+\_\+ptr}, {\ttfamily vector}, {\ttfamily map}, and {\ttfamily optional}, and bidirectional calls between processes through interface pointer and {\ttfamily std\+::function} arguments.

If the wrapped C++ object inherits from an abstract base class declaring virtual methods, the generated {\ttfamily Proxy\+Client} objects can inherit from the same class, allowing interprocess calls to replace local calls without changes to existing code.

There is also optional support for thread mapping, so each thread making interprocess calls can have a dedicated thread processing requests from it, and callbacks from processing threads are executed on corresponding request threads (so recursive mutexes and thread names function as expected in callbacks).

Libmultiprocess acts as a pure wrapper or layer over the underlying protocol. Clients and servers written in other languages, but using a shared capnproto schema can communicate with interprocess counterparties using libmultiprocess without having to use libmultiprocess themselves or having to know about the implementation details of libmultiprocess.\doxysubsection{Internals}\label{md_src_2ipc_2libmultiprocess_2doc_2design_autotoc_md15}
The {\ttfamily Proxy\+Client} and {\ttfamily Proxy\+Server} generated classes are not directly exposed to the user, as described in \doxysectref{usage.md}{p.}{md_src_2ipc_2libmultiprocess_2doc_2usage}{0}. Instead, they wrap C++ interfaces and appear to the user as pointers to an interface. They are first instantiated when calling {\ttfamily Connect\+Stream} and {\ttfamily Serve\+Stream} respectively for creating the {\ttfamily Init\+Interface}. These methods establish connections through sockets, internally creating {\ttfamily Connection} objects wrapping a {\ttfamily capnp\+::\+Rpc\+System} configured for client and server mode respectively.

The {\ttfamily Init\+Interface} interface will typically have methods which return other interfaces, giving the connecting process the ability to call other functions in the serving process. Interfaces can also have methods accepting other interfaces as parameters, giving serving processes the ability to call back and invoke functions in connecting processes. Creating new interfaces does not create new connections, and typically many interface objects will share the same connection.

Both {\ttfamily Connect\+Stream} and {\ttfamily Serve\+Stream} also require an instantiation of the {\ttfamily Event\+Loop}. The {\ttfamily Event\+Loop} owns pending requests, notifies on request dispatch, allows clients from multiple threads to make synchronous calls, and handles some cleanup routines on exit. It must be run in a separate thread so it is always active and can process incoming requests from local clients and remote connections.

When a generated method on the {\ttfamily Proxy\+Client} is called, it calls {\ttfamily client\+Invoke} with the capnp-\/translated types. {\ttfamily client\+Invoke} creates a self-\/executing promise ({\ttfamily kj\+::\+Task\+Set}) that drives the execution of the request and gives ownership of it to the {\ttfamily Event\+Loop}. {\ttfamily client\+Invoke} blocks until a response is received, or until there is a call from the server that needs to run on the same client thread, using a {\ttfamily Waiter} object.

On the server side, the {\ttfamily capnp\+::\+Rpc\+System} receives the capnp request and invokes the corresponding C++ method through the corresponding {\ttfamily Proxy\+Server} and the heavily templated {\ttfamily server\+Invoke} triggering a {\ttfamily Server\+Call}. The return values from the actual C++ methods are copied into capnp responses by {\ttfamily Server\+Ret} and exceptions are caught and copied by {\ttfamily Server\+Except}. The two are connected through {\ttfamily Server\+Field}. The main method driving execution of a request is {\ttfamily Pass\+Field}, which is invoked through {\ttfamily Server\+Field}. Instantiated interfaces, or capabilities in capnp speak, are tracked and owned by the server\textquotesingle{}s {\ttfamily capnp\+::\+Rpc\+System}.\doxysection{Interface descriptions}\label{md_src_2ipc_2libmultiprocess_2doc_2design_autotoc_md16}
As explained in the \doxysectref{usage}{p.}{md_src_2ipc_2libmultiprocess_2doc_2usage}{0} document, interface descriptions need to be consumed both by the {\itshape libmultiprocess} code generator, and by C++ code that calls and implements the interfaces. The C++ code only needs to know about C++ arguments and return types, while the code generator only needs to know about capnp arguments and return types, but both need to know class and method names, so the corresponding {\ttfamily .h} and {\ttfamily .capnp} source files contain some of the same information, and have to be kept in sync manually when methods or parameters change. Despite the redundancy, reconciling the interface definitions is designed to be {\itshape straightforward} and {\itshape safe}. {\itshape Straightforward} because there is no need to write manual serialization code or use awkward intermediate types like {\texttt{{\ttfamily Uni\+Value}}} instead of native types. {\itshape Safe} because if there are any inconsistencies between API and data definitions (even minor ones like using a narrow int data type for a wider int API input), there are errors at build time instead of errors or bugs at runtime.

In the future, it would be possible to combine API and data definitions together using {\texttt{C++ attributes}}. To do this we would add attributes to the API definition files, and then generate the data definitions from the API definitions and attributes. I didn\textquotesingle{}t take this approach mostly because it would be extra work, but also because until C++ standardizes reflection, this would require either hooking into compiler APIs like {\texttt{https\+://github.\+com/\+Rosetta\+Commons/binder}}, or parsing C++ code manually like {\texttt{http\+://www.\+swig.\+org/}}.\doxysection{What is {\ttfamily kj}?}\label{md_src_2ipc_2libmultiprocess_2doc_2design_autotoc_md17}
KJ is a concurrency framework {\texttt{bundled with capnproto}}; it is used as a basis in this library to construct the event-\/loop necessary to service IPC requests.\doxysection{Future directions}\label{md_src_2ipc_2libmultiprocess_2doc_2design_autotoc_md18}
{\itshape libmultiprocess} uses the {\texttt{Cap\textquotesingle{}n Proto}} interface description language and protocol, but it could be extended or changed to use a different IDL/protocol like {\texttt{g\+RPC}}. The nice thing about {\itshape Cap\textquotesingle{}n Proto} compared to {\itshape g\+RPC} and most other lower level protocols is that it allows interface pointers ({\itshape Services} in g\+RPC parlance) to be passed as method arguments and return values, so object references and bidirectional requests work out of the box. Supporting a lower-\/level protocol would require adding maps and tracking code to proxy objects.

{\itshape libmultiprocess} is currently compatible with sandboxing but could add platform-\/specific sandboxing support or integration with a sandboxing library like {\texttt{SAPI}}. 