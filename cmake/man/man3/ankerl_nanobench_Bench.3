.TH "Bench" 3 "Version 2.8" "bitcoind" \" -*- nroff -*-
.ad l
.nh
.SH NAME
Bench \- Main entry point to nanobench's benchmarking facility\&.  

.SH SYNOPSIS
.br
.PP
.PP
\fR#include <nanobench\&.h>\fP
.SS "Public Member Functions"

.in +1c
.ti -1c
.RI "\fBBench\fP ()"
.br
.RI "Creates a new benchmark for configuration and running of benchmarks\&. "
.ti -1c
.RI "\fBBench\fP (Bench &&other) noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBoperator=\fP (\fBBench\fP &&other) noexcept(\fBANKERL_NANOBENCH\fP(NOEXCEPT_STRING_MOVE))"
.br
.ti -1c
.RI "\fBBench\fP (Bench const &other)"
.br
.ti -1c
.RI "\fBBench\fP & \fBoperator=\fP (\fBBench\fP const &other)"
.br
.ti -1c
.RI "\fB~Bench\fP () noexcept"
.br
.ti -1c
.RI "template<typename Op> \fBBench\fP & \fBrun\fP (char const *benchmarkName, Op &&op)"
.br
.RI "Repeatedly calls \fRop()\fP based on the configuration, and performs measurements\&. "
.ti -1c
.RI "template<typename Op> \fBBench\fP & \fBrun\fP (std::string const &benchmarkName, Op &&op)"
.br
.ti -1c
.RI "template<typename Op> \fBBench\fP & \fBrun\fP (Op &&op)"
.br
.RI "Same as run(char const* benchmarkName, Op op), but instead uses the previously set name\&. "
.ti -1c
.RI "\fBBench\fP & \fBtitle\fP (char const *benchmarkTitle)"
.br
.RI "Title of the benchmark, will be shown in the table header\&. Changing the title will start a new markdown table\&. "
.ti -1c
.RI "\fBBench\fP & \fBtitle\fP (std::string const &benchmarkTitle)"
.br
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBname\fP (char const *benchmarkName)"
.br
.RI "Gets the title of the benchmark\&. "
.ti -1c
.RI "\fBBench\fP & \fBname\fP (std::string const &benchmarkName)"
.br
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBcontext\fP (char const *variableName, char const *variableValue)"
.br
.RI "Set context information\&. "
.ti -1c
.RI "\fBBench\fP & \fBcontext\fP (std::string const &variableName, std::string const &variableValue)"
.br
.ti -1c
.RI "\fBBench\fP & \fBclearContext\fP ()"
.br
.RI "Reset context information\&. "
.ti -1c
.RI "template<typename \fBT\fP> \fBBench\fP & \fBbatch\fP (\fBT\fP b) noexcept"
.br
.RI "Sets the batch size\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) double \fBbatch\fP() const noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBunit\fP (char const *unit)"
.br
.RI "Sets the operation unit\&. "
.ti -1c
.RI "\fBBench\fP & \fBunit\fP (std::string const &unit)"
.br
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBtimeUnit\fP (std::chrono::duration< double > const &tu, std::string const &tuName)"
.br
.RI "Sets the time unit to be used for the default output\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBoutput\fP (std::ostream *outstream) noexcept"
.br
.RI "Set the output stream where the resulting markdown table will be printed to\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBclockResolutionMultiple\fP (size_t multiple) noexcept"
.br
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) size_t \fBclockResolutionMultiple\fP() const noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBepochs\fP (size_t numEpochs) noexcept"
.br
.RI "Controls number of epochs, the number of measurements to perform\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) size_t \fBepochs\fP() const noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBmaxEpochTime\fP (std::chrono::nanoseconds t) noexcept"
.br
.RI "Upper limit for the runtime of each epoch\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBminEpochTime\fP (std::chrono::nanoseconds t) noexcept"
.br
.RI "Minimum time each epoch should take\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBminEpochIterations\fP (uint64_t numIters) noexcept"
.br
.RI "Sets the minimum number of iterations each epoch should take\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) uint64_t \fBminEpochIterations\fP() const noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBepochIterations\fP (uint64_t numIters) noexcept"
.br
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) uint64_t \fBepochIterations\fP() const noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBwarmup\fP (uint64_t numWarmupIters) noexcept"
.br
.RI "Sets a number of iterations that are initially performed without any measurements\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) uint64_t \fBwarmup\fP() const noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBrelative\fP (bool isRelativeEnabled) noexcept"
.br
.RI "Marks the next run as the baseline\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) bool \fBrelative\fP() const noexcept"
.br
.ti -1c
.RI "\fBBench\fP & \fBperformanceCounters\fP (bool showPerformanceCounters) noexcept"
.br
.RI "Enables/disables performance counters\&. "
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) bool \fBperformanceCounters\fP() const noexcept"
.br
.ti -1c
.RI "template<typename Arg> \fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & \fBdoNotOptimizeAway\fP (Arg &&arg)"
.br
.RI "Retrieves all benchmark results collected by the bench object so far\&. "
.ti -1c
.RI "template<typename \fBT\fP> \fBBench\fP & \fBcomplexityN\fP (\fBT\fP n) noexcept"
.br
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) double \fBcomplexityN\fP() const noexcept"
.br
.ti -1c
.RI "std::vector< \fBBigO\fP > \fBcomplexityBigO\fP () const"
.br
.ti -1c
.RI "template<typename Op> \fBBigO\fP \fBcomplexityBigO\fP (char const *\fBname\fP, Op op) const"
.br
.RI "Calculates bigO for a custom function\&. "
.ti -1c
.RI "template<typename Op> \fBBigO\fP \fBcomplexityBigO\fP (std::string const &\fBname\fP, Op op) const"
.br
.ti -1c
.RI "\fBBench\fP & \fBrender\fP (char const *templateContent, std::ostream &os)"
.br
.ti -1c
.RI "\fBBench\fP & \fBrender\fP (std::string const &templateContent, std::ostream &os)"
.br
.ti -1c
.RI "\fBBench\fP & \fBconfig\fP (\fBConfig\fP const &benchmarkConfig)"
.br
.ti -1c
.RI "\fBANKERL_NANOBENCH\fP (NODISCARD) \fBConfig\fP const &\fBconfig\fP() const noexcept"
.br
.ti -1c
.RI "template<typename Arg> \fBBench\fP & \fBdoNotOptimizeAway\fP (Arg &&arg)"
.br
.in -1c
.SH "Detailed Description"
.PP 
Main entry point to nanobench's benchmarking facility\&. 

It holds configuration and results from one or more benchmark runs\&. Usually it is used in a single line, where the object is constructed, configured, and then a benchmark is run\&. E\&.g\&. like this: 
.PP
.nf
ankerl::nanobench::Bench()\&.unit("byte")\&.batch(1000)\&.run("random fluctuations", [&] {
    // here be the benchmark code
});

.fi
.PP

.PP
In that example \fBBench()\fP constructs the benchmark, it is then configured with \fBunit()\fP and \fBbatch()\fP, and after configuration a benchmark is executed with \fBrun()\fP\&. Once \fBrun()\fP has finished, it prints the result to \fRstd::cout\fP\&. It would also store the results in the \fBBench\fP instance, but in this case the object is immediately destroyed so it's not available any more\&. 
.SH "Constructor & Destructor Documentation"
.PP 
.SS "Bench ()"

.PP
Creates a new benchmark for configuration and running of benchmarks\&. 
.SS "Bench (Bench && other)\fR [noexcept]\fP"

.SS "Bench (Bench const & other)"

.SS "~\fBBench\fP ()\fR [noexcept]\fP"

.SH "Member Function Documentation"
.PP 
.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const &\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "ANKERL_NANOBENCH (NODISCARD ) const\fR [noexcept]\fP"

.SS "template<typename \fBT\fP> \fBBench\fP & batch (\fBT\fP b)\fR [noexcept]\fP"

.PP
Sets the batch size\&. E\&.g\&. number of processed byte, or some other metric for the size of the processed data in each iteration\&. If you benchmark hashing of a 1000 byte long string and want byte/sec as a result, you can specify 1000 as the batch size\&.

.PP
\fBTemplate Parameters\fP
.RS 4
\fIT\fP Any input type is internally cast to \fRdouble\fP\&. 
.RE
.PP
\fBParameters\fP
.RS 4
\fIb\fP batch size 
.RE
.PP

.SS "\fBBench\fP & clearContext ()"

.PP
Reset context information\&. This may improve efficiency when using many context entries, or improve robustness by removing spurious context entries\&.

.PP
\fBSee also\fP
.RS 4
\fBcontext\fP 
.RE
.PP

.SS "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & clockResolutionMultiple (size_t multiple)\fR [noexcept]\fP"
Modern processors have a very accurate clock, being able to measure as low as 20 nanoseconds\&. This is the main trick nanobech to be so fast: we find out how accurate the clock is, then run the benchmark only so often that the clock's accuracy is good enough for accurate measurements\&.

.PP
The default is to run one epoch for 1000 times the clock resolution\&. So for 20ns resolution and 11 epochs, this gives a total runtime of

.PP
\[20ns * 1000 * 11 \approx 0.2ms
\]
.PP
To be precise, nanobench adds a 0-20% random noise to each evaluation\&. This is to prevent any aliasing effects, and further improves accuracy\&.

.PP
Total runtime will be higher though: Some initial time is needed to find out the target number of iterations for each epoch, and there is some overhead involved to start & stop timers and calculate resulting statistics and writing the output\&.

.PP
\fBParameters\fP
.RS 4
\fImultiple\fP Target number of times of clock resolution\&. Usually 1000 is a good compromise between runtime and accuracy\&. 
.RE
.PP

.SS "std::vector< \fBBigO\fP > complexityBigO () const"
Calculates \fRBig O\fP of the results with all preconfigured complexity functions\&. Currently these complexity functions are fitted into the benchmark results:

.PP
$ \mathcal{O}(1) $, $ \mathcal{O}(n) $, $ \mathcal{O}(\log{}n) $, $ \mathcal{O}(n\log{}n) $, $ \mathcal{O}(n^2) $, $ \mathcal{O}(n^3) $\&.

.PP
If we e\&.g\&. evaluate the complexity of \fRstd::sort\fP, this is the result of \fRstd::cout << bench\&.complexityBigO()\fP:

.PP
.PP
.nf
|   coefficient |   err% | complexity
|\-\-\-\-\-\-\-\-\-\-\-\-\-\-:|\-\-\-\-\-\-\-:|\-\-\-\-\-\-\-\-\-\-\-\-
|   5\&.08935e\-09 |   2\&.6% | O(n log n)
|   6\&.10608e\-08 |   8\&.0% | O(n)
|   1\&.29307e\-11 |  47\&.2% | O(n^2)
|   2\&.48677e\-15 |  69\&.6% | O(n^3)
|   9\&.88133e\-06 | 132\&.3% | O(log n)
|   5\&.98793e\-05 | 162\&.5% | O(1)
.fi
.PP

.PP
So in this case $ \mathcal{O}(n\log{}n) $ provides the best approximation\&.

.PP
.PP
.nf
embed:rst
See the tutorial :ref:`asymptotic-complexity` for details\&.
.fi
.PP
 
.PP
\fBReturns\fP
.RS 4
Evaluation results, which can be printed or otherwise inspected\&. 
.RE
.PP

.SS "template<typename Op> \fBBigO\fP complexityBigO (char const * name, Op op) const"

.PP
Calculates bigO for a custom function\&. E\&.g\&. to calculate the mean squared error for $ \mathcal{O}(\log{}\log{}n) $, which is not part of the default set of \fBcomplexityBigO()\fP, you can do this:

.PP
.PP
.nf
auto logLogN = bench\&.complexityBigO("O(log log n)", [](double n) {
    return std::log2(std::log2(n));
});
.fi
.PP

.PP
The resulting mean squared error can be printed with \fRstd::cout << logLogN\fP\&. E\&.g\&. it prints something like this:

.PP
.PP
.nf
2\&.46985e\-05 * O(log log n), rms=1\&.48121
.fi
.PP

.PP
\fBTemplate Parameters\fP
.RS 4
\fIOp\fP Type of mapping operation\&. 
.RE
.PP
\fBParameters\fP
.RS 4
\fIname\fP Name for the function, e\&.g\&. "O(log log n)" 
.br
\fIop\fP Op's operator() maps a \fRdouble\fP with the desired complexity function, e\&.g\&. \fRlog2(log2(n))\fP\&. 
.RE
.PP
\fBReturns\fP
.RS 4
\fBBigO\fP Error calculation, which is streamable to std::cout\&. 
.RE
.PP

.SS "template<typename Op> \fBBigO\fP complexityBigO (std::string const & name, Op op) const"

.SS "template<typename \fBT\fP> \fBBench\fP & complexityN (\fBT\fP n)\fR [noexcept]\fP"

.PP
.nf
embed:rst

Sets N for asymptotic complexity calculation, so it becomes possible to calculate `Big O
<https://en\&.wikipedia\&.org/wiki/Big_O_notation>`_ from multiple benchmark evaluations\&.

Use :cpp:func:`ankerl::nanobench::Bench::complexityBigO` when the evaluation has finished\&. See the tutorial
:ref:`asymptotic-complexity` for details\&.

.fi
.PP

.PP
\fBTemplate Parameters\fP
.RS 4
\fIT\fP Any type is cast to \fRdouble\fP\&. 
.RE
.PP
\fBParameters\fP
.RS 4
\fIn\fP Length of N for the next benchmark run, so it is possible to calculate \fRbigO\fP\&. 
.RE
.PP

.SS "\fBBench\fP & config (\fBConfig\fP const & benchmarkConfig)"

.SS "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & context (char const * variableName, char const * variableValue)"

.PP
Set context information\&. The information can be accessed using custom render templates via \fR{{context(variableName)}}\fP\&. Trying to render a variable that hasn't been set before raises an exception\&. Not included in (default) markdown table\&.

.PP
\fBSee also\fP
.RS 4
\fBclearContext\fP, \fBrender\fP
.RE
.PP
\fBParameters\fP
.RS 4
\fIvariableName\fP The name of the context variable\&. 
.br
\fIvariableValue\fP The value of the context variable\&. 
.RE
.PP

.SS "\fBBench\fP & context (std::string const & variableName, std::string const & variableValue)"

.SS "template<typename Arg> \fBBench\fP & doNotOptimizeAway (Arg && arg)"

.SS "template<typename Arg> \fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & doNotOptimizeAway (Arg && arg)"

.PP
Retrieves all benchmark results collected by the bench object so far\&. Each call to \fBrun()\fP generates a \fBResult\fP that is stored within the \fBBench\fP instance\&. This is mostly for advanced users who want to see all the nitty gritty details\&.

.PP
\fBReturns\fP
.RS 4
All results collected so far\&.
.RE
.PP
.PP
.nf
embed:rst

Convenience shortcut to :cpp:func:`ankerl::nanobench::doNotOptimizeAway`\&.
.fi
.PP
 
.SS "\fBBench\fP & epochIterations (uint64_t numIters)\fR [noexcept]\fP"
Sets exactly the number of iterations for each epoch\&. Ignores all other epoch limits\&. This forces nanobench to use exactly the given number of iterations for each epoch, not more and not less\&. Default is 0 (disabled)\&.

.PP
\fBParameters\fP
.RS 4
\fInumIters\fP Exact number of iterations to use\&. Set to 0 to disable\&. 
.RE
.PP

.SS "\fBBench\fP & epochs (size_t numEpochs)\fR [noexcept]\fP"

.PP
Controls number of epochs, the number of measurements to perform\&. The reported result will be the median of evaluation of each epoch\&. The higher you choose this, the more deterministic the result be and outliers will be more easily removed\&. Also the \fRerr%\fP will be more accurate the higher this number is\&. Note that the \fRerr%\fP will not necessarily decrease when number of epochs is increased\&. But it will be a more accurate representation of the benchmarked code's runtime stability\&.

.PP
Choose the value wisely\&. In practice, 11 has been shown to be a reasonable choice between runtime performance and accuracy\&. This setting goes hand in hand with \fBminEpochIterations()\fP (or \fBminEpochTime()\fP)\&. If you are more interested in \fImedian\fP runtime, you might want to increase \fBepochs()\fP\&. If you are more interested in \fImean\fP runtime, you might want to increase \fBminEpochIterations()\fP instead\&.

.PP
\fBParameters\fP
.RS 4
\fInumEpochs\fP Number of epochs\&. 
.RE
.PP

.SS "\fBBench\fP & maxEpochTime (std::chrono::nanoseconds t)\fR [noexcept]\fP"

.PP
Upper limit for the runtime of each epoch\&. As a safety precaution if the clock is not very accurate, we can set an upper limit for the maximum evaluation time per epoch\&. Default is 100ms\&. At least a single evaluation of the benchmark is performed\&.

.PP
\fBSee also\fP
.RS 4
\fBminEpochTime\fP, \fBminEpochIterations\fP
.RE
.PP
\fBParameters\fP
.RS 4
\fIt\fP Maximum target runtime for a single epoch\&. 
.RE
.PP

.SS "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & minEpochIterations (uint64_t numIters)\fR [noexcept]\fP"

.PP
Sets the minimum number of iterations each epoch should take\&. Default is 1, and we rely on \fBclockResolutionMultiple()\fP\&. If the \fRerr%\fP is high and you want a more smooth result, you might want to increase the minimum number of iterations, or increase the \fBminEpochTime()\fP\&.

.PP
\fBSee also\fP
.RS 4
\fBminEpochTime\fP, \fBmaxEpochTime\fP, \fBminEpochIterations\fP
.RE
.PP
\fBParameters\fP
.RS 4
\fInumIters\fP Minimum number of iterations per epoch\&. 
.RE
.PP

.SS "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & minEpochTime (std::chrono::nanoseconds t)\fR [noexcept]\fP"

.PP
Minimum time each epoch should take\&. Default is zero, so we are fully relying on \fBclockResolutionMultiple()\fP\&. In most cases this is exactly what you want\&. If you see that the evaluation is unreliable with a high \fRerr%\fP, you can increase either \fBminEpochTime()\fP or \fBminEpochIterations()\fP\&.

.PP
\fBSee also\fP
.RS 4
\fBmaxEpochTime\fP, \fBminEpochIterations\fP
.RE
.PP
\fBParameters\fP
.RS 4
\fIt\fP Minimum time each epoch should take\&. 
.RE
.PP

.SS "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & name (char const * benchmarkName)"

.PP
Gets the title of the benchmark\&. Name of the benchmark, will be shown in the table row\&. 
.SS "\fBBench\fP & name (std::string const & benchmarkName)"

.SS "\fBBench\fP & operator= (\fBBench\fP && other)\fR [noexcept]\fP"

.SS "\fBBench\fP & operator= (\fBBench\fP const & other)"

.SS "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & output (std::ostream * outstream)\fR [noexcept]\fP"

.PP
Set the output stream where the resulting markdown table will be printed to\&. The default is \fR&std::cout\fP\&. You can disable all output by setting \fRnullptr\fP\&.

.PP
\fBParameters\fP
.RS 4
\fIoutstream\fP Pointer to output stream, can be \fRnullptr\fP\&. 
.RE
.PP

.SS "\fBBench\fP & performanceCounters (bool showPerformanceCounters)\fR [noexcept]\fP"

.PP
Enables/disables performance counters\&. On Linux nanobench has a powerful feature to use performance counters\&. This enables counting of retired instructions, count number of branches, missed branches, etc\&. On default this is enabled, but you can disable it if you don't need that feature\&.

.PP
\fBParameters\fP
.RS 4
\fIshowPerformanceCounters\fP True to enable, false to disable\&. 
.RE
.PP

.SS "\fBBench\fP & relative (bool isRelativeEnabled)\fR [noexcept]\fP"

.PP
Marks the next run as the baseline\&. Call \fRrelative(true)\fP to mark the run as the baseline\&. Successive runs will be compared to this run\&. It is calculated by

.PP
\[100\% * \frac{baseline}{runtime}
\]
.PP
.IP "\(bu" 2
100% means it is exactly as fast as the baseline
.IP "\(bu" 2
>100% means it is faster than the baseline\&. E\&.g\&. 200% means the current run is twice as fast as the baseline\&.
.IP "\(bu" 2
<100% means it is slower than the baseline\&. E\&.g\&. 50% means it is twice as slow as the baseline\&.
.PP

.PP
See the tutorial section "Comparing Results" for example usage\&.

.PP
\fBParameters\fP
.RS 4
\fIisRelativeEnabled\fP True to enable processing 
.RE
.PP

.SS "\fBBench\fP & render (char const * templateContent, std::ostream & os)"

.PP
.nf
embed:rst

Convenience shortcut to :cpp:func:`ankerl::nanobench::render`\&.

.fi
.PP
 
.SS "\fBBench\fP & render (std::string const & templateContent, std::ostream & os)"

.SS "template<typename Op> \fBBench\fP & run (char const * benchmarkName, Op && op)"

.PP
Repeatedly calls \fRop()\fP based on the configuration, and performs measurements\&. This call is marked with \fRnoinline\fP to prevent the compiler to optimize beyond different benchmarks\&. This can have quite a big effect on benchmark accuracy\&.

.PP
.PP
.nf
embed:rst
\&.\&. note::

  Each call to your lambda must have a side effect that the compiler can't possibly optimize it away\&. E\&.g\&. add a result to an
  externally defined number (like `x` in the above example), and finally call `doNotOptimizeAway` on the variables the compiler
  must not remove\&. You can also use :cpp:func:`ankerl::nanobench::doNotOptimizeAway` directly in the lambda, but be aware that
  this has a small overhead\&.
.fi
.PP

.PP
\fBTemplate Parameters\fP
.RS 4
\fIOp\fP The code to benchmark\&. 
.RE
.PP

.SS "template<typename Op> \fBBench\fP & run (Op && op)"

.PP
Same as run(char const* benchmarkName, Op op), but instead uses the previously set name\&. 
.PP
\fBTemplate Parameters\fP
.RS 4
\fIOp\fP The code to benchmark\&. 
.RE
.PP

.SS "template<typename Op> \fBBench\fP & run (std::string const & benchmarkName, Op && op)"

.SS "\fBANKERL_NANOBENCH\fP(NODISCARD) \fBstd\fP \fBBench\fP & timeUnit (std::chrono::duration< double > const & tu, std::string const & tuName)"

.PP
Sets the time unit to be used for the default output\&. Nanobench defaults to using ns (nanoseconds) as output in the markdown\&. For some benchmarks this is too coarse, so it is possible to configure this\&. E\&.g\&. use \fR\fBtimeUnit\fP(1ms, "ms")\fP to show \fRms/op\fP instead of \fRns/op\fP\&.

.PP
\fBParameters\fP
.RS 4
\fItu\fP Time unit to display the results in, default is 1ns\&. 
.br
\fItuName\fP Name for the time unit, default is "ns" 
.RE
.PP

.SS "\fBBench\fP & title (char const * benchmarkTitle)"

.PP
Title of the benchmark, will be shown in the table header\&. Changing the title will start a new markdown table\&. 
.PP
\fBParameters\fP
.RS 4
\fIbenchmarkTitle\fP The title of the benchmark\&. 
.RE
.PP

.SS "\fBBench\fP & title (std::string const & benchmarkTitle)"

.SS "\fBBench\fP & unit (char const * unit)"

.PP
Sets the operation unit\&. Defaults to "op"\&. Could be e\&.g\&. "byte" for string processing\&. This is used for the table header, e\&.g\&. to show \fRns/byte\fP\&. Use singular (\fIbyte\fP, not \fIbytes\fP)\&. A change clears the currently collected results\&.

.PP
\fBParameters\fP
.RS 4
\fIunit\fP The unit name\&. 
.RE
.PP

.SS "\fBBench\fP & unit (std::string const & unit)"

.SS "\fBBench\fP & warmup (uint64_t numWarmupIters)\fR [noexcept]\fP"

.PP
Sets a number of iterations that are initially performed without any measurements\&. Some benchmarks need a few evaluations to warm up caches / database / whatever access\&. Normally this should not be needed, since we show the median result so initial outliers will be filtered away automatically\&. If the warmup effect is large though, you might want to set it\&. Default is 0\&.

.PP
\fBParameters\fP
.RS 4
\fInumWarmupIters\fP Number of warmup iterations\&. 
.RE
.PP


.SH "Author"
.PP 
Generated automatically by Doxygen for bitcoind from the source code\&.
